\input{mmd-scrivcustom-header}
\input{mmd-scrivcustom-begin-doc}

\chapter{Chapter 1: Introduction}
\label{chapter1:introduction}

Bilinguals who are highly proficient in both languages seem to be able to operate in one of their languages independently without influence from other languages. With the exception of perhaps a slight accent or subtle differences in wording, the speech of a highly proficient bilingual may be indistinguishable from that of a monolingual speaker. All language users, monolingual, bilingual, multilingual, occasionally make speech errors, mixing up the order of words in a sentence, for example. Bilinguals however, very rarely slip up and use a word in the wrong language for the situation. These observations could be taken as evidence that bilinguals can functionally separate their two languages, perhaps representing the two languages in independent stores. Indeed, the narrative in the literature on child bilingual language acquisition is that children begin life with a merged language system and eventually learn to separate the two languages, and some early brain imaging evidence suggests that the languages may be represented in distinct areas. However, psycholinguistic research on adult bilinguals who are fluent in two languages has shown that the two languages do constantly interact. When a bilingual reads a word in one language, related features are co-activated in the unintended language, and when a bilingual prepares to speak a word, the other language translation equivalent is on the tip of their tongue. As such, models of the bilingual lexicon that have been proposed in the past decade posit integrated representational stores for the two languages. Likewise in the domain of syntax, bilinguals can be primed to use a syntactic structure even, if that structure was heard in a different language. This suggests that syntax is also shared across languages and on some level becomes co-activated. Models of bilingual syntactic representation also posit shared storage of syntactic structures. Yet, the question remains: if the two languages are co-activated and interact, how does a bilingual or multilingual finally select a word or structure in the intended language from the myriad of co-activated alternatives? 

This dissertation addresses the question of how bilinguals select the language they intend to use at the lexical level and at the syntactic level. The hypothesis explored here is that features of a sentence that are distinct to one language (specifically, when syntactic constructions differ across the two language) allow bilinguals to functionally separate syntactic structures and predict the language of upcoming words in a sentence. This question is addressed through the novel combination of two distinct research paradigms: the confederate picture description task and the Rapid Serial Visual Presentation reading paradigm. The confederate picture description task measures cross-language priming (i.e., the propensity to repeat a syntactic construction, such as the active construction or the passive construction, that was used recently), providing an index of the functional separability of various syntactic constructions within a population of bilinguals. When structures are primed across languages, they are assumed to be represented in a common, language-general store. If they are not primed, they are assumed to be represented in a distinct, language-specific store. The results of previous confederate picture description studies conducted on bilinguals who speak a variety of language pairs indicate that structures which share word order across the two languages are represented in language-general stores and structures that do not share word order are represented in language-specific stores. In Experiment 1 of this dissertation, the implications of these findings are applied to the domain of word recognition during reading: two sets of sentences are constructed: (1) a set of sentences with syntactic structures that are inferred to be language-specific for Spanish-English bilinguals and (2) a set of sentences with syntactic structures that are inferred to be language-general for Spanish-English bilinguals. The goal of Experiment 1 is to determine whether language co-activation in Spanish-English bilinguals is differential under these two sets of structures. This approach is similar to the one taken in the realm of corpus linguistics, where researchers use facts about language usage to make inferences about language representation. In Experiment 2, a confederate picture description study is conducted on a similar sample of Spanish-English bilinguals, providing independent evidence on whether the inferences of syntactic storage hold for this sample of bilinguals.

To preview the findings{\ldots}{\ldots} [Results]. [General Conclusions]

The dissertation is laid out as follows. In what remains of Chapter\ensuremath{\sim}\textbackslash{}ref\{chap:Intro\}, I provide an overview of the evidence for parallel activation of two languages in bilinguals, focusing on the lexical level. I then discuss the extant research constraints to language co-activation, including whether aspects of sentence context affect co-activation. In Chapter\ensuremath{\sim}\textbackslash{}ref\{chap:RoadMap\} I provide an intellectual and methodical roadmap to the empirical investigations in Chapters \textbackslash{}ref\{chap:RSVP\} and \textbackslash{}ref\{chap:Priming\}. Specifically, I review the literature on cross-language syntactic priming and argue that data from language usage can provide valuable insights into how syntactic structure is stored representationally. In Chapter\ensuremath{\sim}\textbackslash{}ref\{chap:RSVP\}, I present the empirical results of Experiment 1 in which I investigate whether syntactic constructions that are implicated as language-specific structures by previous syntactic priming research can reduce co-activation of the unintended language at the lexical level. In Chapter\ensuremath{\sim}\textbackslash{}ref\{chap:Priming\}, I present the empirical results of Experiment 2 in which I investigate whether word-order differences allow bilinguals to establish structures as language-specific. Finally in Chapter\ensuremath{\sim}\textbackslash{}ref\{chap:Discussion\}, I present the general discussion and conclusions, tying together the results of the two empirical studies. 

\section{Parallel activation at the lexical level}
\label{parallelactivationatthelexicallevel}

The strongest evidence for parallel activation of the two languages has been shown when bilinguals read single words that are ambiguous across their two languages, such as interlingual cognates and homographs. Cognates are words which share form and meaning across languages (e.g., \texttt{piano'' in English and Spanish) while homographs overlap in form but conflict in meaning across languages (e.g.,}pan'' which is a receptacle for cooking in English but a leavened food item in Spanish). If it were possible for a bilingual to access words selectively in a single language, then the presence of cross-language ambiguity should not influence processing in comparison to non-ambiguous words. However, contrary to this prediction, cognates typically elicit faster reaction times to read or name the word (i.e., cognate facilitation; e.g., Dijkstra et al., 1998; Van Hell \textbackslash{}\& Dijkstra, 2002; Schwartz et al., 2007). Inter-lingual homographs typically elicit slower processing times (i.e., homograph inhibition; e.g., Beauvillain \textbackslash{}\& Grainger, 1987; Dijkstra et al., 1998). In the past decade, cross-language co-activation has become a widely-accepted phenomenon in the field of bilingualism. Critically, monolingual of either language pair do not show differential effects towards interlingual cognates or homographs, suggesting that the effects witnessed in bilingual participants are due to their bilingualism and not to spurious effects related to uncontrolled properties of the stimuli.

Evidence for language co-activation (or language non-selectivity) is observed across a broad range of tasks and contexts. Cognate facilitation and homograph inhibition are observed using a variety of dependent measures. Initial research was conducted with behavioral measures such as reaction time for lexical decision (Dijkstra et al., 2008; Van Hell \textbackslash{}\& De Groot, 2008; Van Hell \textbackslash{}\& Dijkstra, 2002), translation (Van Hell \textbackslash{}\& De Groot, 2008; Sanchez-Casas, Davis, \textbackslash{}\& Garcia-Albea, 1992), word association (Van Hell \textbackslash{}\& Dijkstra, 2002), and word naming (e.g., Schwartz \textbackslash{}\& Kroll, 2006; Schwartz et al., 2007). More recent studies find language co-activation in measures such as eye-tracking (Duyck et al., 2007; Libben \textbackslash{}\& Titone, 2009; Titone et al., 2011; Van Assche et al., 2010; Van Assche et al., 2009) and event-related potentials (ERPs; Midgley, Holcomb, \textbackslash{}\& Grainger, 2011). Behavioral measures tend to measure the aggregate result of processing. Hence a cognate effect in a lexical decision task provides no information about the point in the timecourse of processing at which both languages started to become activated. It could theoretically be the case that parallel activation occurs late in the process of word recognition, almost as if bilinguals translate words between their two languages, or both languages might become activated initially at the point at which orthography begins to be decoded. More time sensitive methods, such as ERPs and eye-tracking, indicate that parallel activation is not solely a late process. Both language alternatives become activated early in processing and remain activated during late stages of processing. For example, in eye tracking, cognate effects are observable in first fixation duration and gaze duration measures, assumed to reflect initial lexical access. They also show that both languages remain activated throughout the time-course of processing (e.g., Duyck et al., 2007; Libben \textbackslash{}\& Titone, 2009; Van Assche et al., 2010; Van Assche et al., 2009) via cross-language effects in total reading time, a finding which could suggest that the intended language may never actually be selected categorically or that the selection is not observable without an even more sensitive measure. Non-selective access has been observed for language pairs such as Dutch and English, which share the same writing system (e.g., Dijkstra, 2005), in Chinese and English, which do not (Thierry \textbackslash{}\& Wu, 2007), and in English and American Sign Language (e.g., Morford et al., 2011), one spoken and one signed language. Parallel activation is also not simply a side-effect of L2 processing. While the first or dominant language (L1) does strongly influence processing of the weaker L2, a bilingual's L2 or even L3 can become activated during L1 processing (e.g., Van Assche et al., 2009; Van Hell \textbackslash{}\& Dijkstra, 2002). Overall, the degree of cross-language activation is relatively insensitive to the demands of the task at hand. Parallel activation is observed in blocked and mixed language contexts and across a range of different methods including naming latencies (e.g., Schwartz \textbackslash{}\& Kroll, 2006; Schwartz, et al., 2007), eye-movement records (e.g., Duyck et al., 2007, Libben \textbackslash{}\& Titone, 2009; Van Assche et al., 2009; Van Assche et al., 2010), lexical decision times (e.g., Dijkstra et al., 1998; Van Hell \textbackslash{}\& De Groot, 2008; Van Hell \textbackslash{}\& Dijkstra, 2002), and ERPs (e.g., Midgley et al., 2011). 

Parallel activation is a graded phenomenon, not an all-or-nothing process. As in monolingual word recognition (Seidenberg \textbackslash{}\& McClelland, 1989), the activation of any particular word for a bilingual is a continuous function of a distributed pattern of activation across multiple levels of mental representations, including the orthography, phonology, and semantics. Thus, the degree of cross-language overlap at each level factors in to the degree of observable cross-language co-activation. Cognate and homograph effects are larger when words share orthographic overlap or phonological overlap across the two languages. These effects are not categorical but continuous, such that greater overlap corresponds to a larger magnitude of cross-language co-activation(e.g., Duyck, Van Assche, Drieghe, \textbackslash{}\& Hartsuiker, 2007; Schwartz, Kroll, \textbackslash{}\& Diaz., 2007; Van Assche, Drieghe, Duyck, Welvaert, \textbackslash{}\& Hartsuiker, 2010; Van Assche, Duyck, Hartsuiker, \textbackslash{}\& Diependaele, 2009). Parallel activation of the semantics is evident in homograph recognition; the conflict in meaning across the two readings of an interlingual homograph produces a cost to processing. No studies to my knowledge have tested whether the degree of shared semantic overlap across languages influences the degree of co-activation. However, cognate effects in translation have been shown to depend upon semantic factors such as word concreteness (e.g., Van Hell \textbackslash{}\& De Groot, 2008). In sum, bilingual word recognition is an interactive process dependent on graded activation among multiple levels of representation. 

The main focus of work on parallel activation has been on the processing of cognates and homographs. However, there are at least two critiques to this approach. First, one can argue that the processing differences observed for words with cross-language overlap are simply the result of increased frequency of usage of word. For example, a Spanish-English bilingual will experience the cognate word ``bus'' twice as often as her monolingual counterpart, and this increased experience may lead to a processing advantage for that word in the bilingual's lexicon (see e.g. the weaker-links or frequency lag hypotheses, Gollan, Montoya, Cera, \textbackslash{}\& Sandoval, 2008; Gollan, et al., 2011). Second, it is not clear from the study of cognate and homograph processing alone that non-selective access extends to processing of every word in the lexicon. While it is likely true that increased frequency of usage is partially responsible for the cognate and homograph effects, it does not completely rule out parallel activation of languages. Cognate and homograph effects depend on the degree of orthographic overlap (e.g., which can be calculated via the Van Orden or Levenshtein distance methods, Levenshtein, 1966; Van Orden, 1987) and phonological overlap (that can be elicited from participants making auditory judgments on sound overlap) of a word between the two. Smaller cross-language effects are observed for language-ambiguous words with a less orthographic or phonological overlap, and this relationship is linear (e.g., Dijkstra et al., 1999; Schwartz et al., 2007; Van Assche et al., 2010). A purely frequency-dependent hypothesis would not predict sensitivity to cross-language overlap within language-ambiguous words. Touching on the second critique, further evidence in favor of the parallel activation hypothesis is the observation of cross-language effects for stimuli that share no overt similarities between the two languages and in tasks that require no overt language processing (Chabal \textbackslash{}\& Marian, 2013; Morford, Wilkinson, Villwock, Piñar, \textbackslash{}\& Kroll, 2011; Thierry \textbackslash{}\& Wu, 2007; Wu, Cristino, Leek, \textbackslash{}\& Thierry, 2013). For example when proficient Chinese-English bilinguals and monolingual speaker of each language make semantic relatedness judgments on English words, both groups of speakers show semantic priming via a positive modulation of the N400 component for related words. However, only bilinguals show an additional modulation when the Chinese translation of the English words share characters and phonology, features that were never overtly present in the experiment (Thierry \textbackslash{}\& Wu, 2007; Wu \textbackslash{}\& Thierry, 2010). Likewise, deaf signers who read English activate the sign translation equivalents despite the fact that there is no phonological nor orthographic overlap between the two linguistic systems. Thus there is strong evidence for cross-language activation in spite of the frequency dependence and when stimuli other than cognates and homographs are used in experiments. 

\section{Models of bilingual word recognition}
\label{modelsofbilingualwordrecognition}

\textbackslash{}textcite\{Dijkstra2002\} proposed the BIA+ model to account for cross-language interactions during bilingual word recognition (see also \textbackslash{}textcite\{Dijkstra1998\} for an earlier version of the BIA+ model). The BIA+ model, adapted from the Interactive Activation Model \textbackslash{}parencite\{McClelland1981\}, was designed to account for data from reading experiments conducted with bilingual participants. The model, shown in Figure \textbackslash{}ref\{BIA+\}, is divided into two separate levels: the task schema and the word identification system. The word identification system is responsible for handling only linguistic input while the task schema handles the demands of non-linguistic contexts. 

\textbackslash{}begin\{figure\}\textbackslash{}label\{BIA+\}
\textbackslash{}begin\{center\}
\textbackslash{}includegraphics[scale=0.5]\{Intro\slash Figures\slash biaplus.png\}
\textbackslash{}caption\{The BIA+ model of word recognition \textbackslash{}parencite[reprinted from][]\{Dijkstra2002\}\}
\textbackslash{}end\{center\}
\textbackslash{}end\{figure\}

The word identification system deals with linguistic input to the model. The BIA+ posits an integrated lexicon (i.e., the words of each language are integrated into one “dictionary”) and shared semantics across the two languages. The word identification system is highly interactive. Upon “reading” a word, nodes for phonological and orthographic representations at the lexical and sublexical levels become active. Activation then spreads within and between the lexical and sublexical levels causing potential candidates to become more highly activated than other words. The higher levels of the model (i.e., the semantics level and language nodes) receive activation from the lower levels. In the semantic level, concepts receiving enough activation spread this activation back down to the lower levels further reinforcing the activation of potential lexical candidates. The language nodes are responsible for identification of the language being read. A crucial assumption made by the model is that the higher level nodes may only receive bottom-up activation. Furthermore, the language nodes may not send activation back down to the lower levels. Thus, prior knowledge of the intended language will not increase activation to nodes at lower levels. That is, the language nodes cannot function as a language filter. Instead, the nodes must be sufficiently activated through experience with a linguistic input.

While the word identification system handles linguistic input, the task schema deals with non-linguistic contexts. The task schema is responsible for accomplishing the task at hand (e.g., lexical decision, naming, etc.) and determining when a response should be made. In order to help with this decision, this level of the model receives constant input from the word identification system. A critical assumption of the BIA+ model is that the task schema (and, thus, non-linguistic context) does not infiltrate the word recognition system. Evidence for this was shown by \textbackslash{}textcite\{Dijkstra2000\} who demonstrated that the presence of stimuli and not expectations derived from instructions affected bilingual performance in a lexical decision task.

Given the assumptions of the BIA+ model it is easy to see how cross-language overlap will affect the recognition of words. Cognates, because of their close overlap in orthography, phonology, and meaning, will receive activation in both languages more quickly compared to words without similar overlap. Thus lexical decision or naming will be facilitated. On the other hand, when homographs are the input, the cross-language overlap with orthography and phonology may initially speed activation but the discrepancy in meaning will cause the system to have trouble identifying the language of the word thus slowing lexical decision or naming performance. Overall, the BIA+ predicts that a parallel access account with respect to language occurs in a bottom-up fashion. This parallel activity is not easily constrained by non-linguistic contexts. 

The question now is how linguistic contexts influence word recognition. Because the BIA+ model was designed to account for word recognition outside of sentence contexts, it makes no explicit predictions regarding linguistic contexts. However, as we shall see in the following section, there are specific linguistic contexts that may allow bilinguals to recognize words in a language-selective manner.

\section{Constraints on language co-activation}
\label{constraintsonlanguageco-activation}

A major question in multilingual word recognition is whether a single language alternative can be selected from the myriad of activated words, and if so, what cues may aid in enabling a language selection. The recent research shows that language-selective access is difficult to achieve. \textbackslash{}textem\{Preview this section\}

Aspects of lexical form

One potential source for a language cue may be present in the cross-linguistic differences between two language pairs; languages often differ on many facets. Language specific characteristics can be quantified on many different levels of representation: phonological, orthographic, morphosyntactic, syntactic, pragmatic, etc. Languages may have different allowable phonemes (e.g., click languages), phoneme that are perceived as different in one language because they tend to mark a difference in meaning may be realized as the same in another language where they do not differentiate meaning. Languages exhibit different phonotactics where e.g., consonant clusters may be allowed in onset positon for some languages (e.g., tk in XXX but not in English). Language have different orthographic systems. They may share script, e.g. English, Spanish, and German share much of the same latin script, but there are differences in diacritics. Languages may share completely separate as in latin, Cyrillic, and Arabic. Morphosyntactically, some languages exhibit characteristics not seen in other languages. For example, many romance languages such as Spanish have a robust system of pronomical object clitics (or proclitics, e.g., “le blah blah”) that does not exist in modern Germanic languages such as English or German. Syntactically, the study of word order has been very important in the field of linguistics, particularly with the advent of Chomsky's idea of universal, generative grammar in the late 1950s. Languages readily exhibit differing word orders, and much effort has been invested into unifying these differences. This includes language typologists who have focused on constituent order and differences across language. For example, Greenberg (1963) proposes a basic order typology from which he derives a set of linguistic universals: languages tend to have prepositions or postpositions, one of the six possible orderings of the subject, verb, and object, and adjectives that occur prenominally or postnominally. The position of qualifying adjectives in relation to nouns. (From a UG standpoint, maybe there are relatively few real differences in how language is structures across different languages).

Cognates, for example, often exhibit slight differences in their realization between two languages, despite their overall shared form. Cognates often have distinct phonology in each language (e.g., the cognates base and base in English and Spanish) and can also lack perfect orthographic overlap (e.g., the cognates ship and schip in English and Dutch). Likewise, cognate translations related by phonology may be written in entirely different scripts, as is the case between Hebrew and English or Chinese and English. In the most extreme case, one of the two languages in a pair may lack a system of writing entirely such as is the case for ASL-English bilinguals. Counterintuitively, these structural differences do appear not to function as a language cue. As mentioned earlier, Morford et al. (2011) found that ASL-English bilinguals were facilitated in judging that two English words were semantically related if the two words also shared similar hand-shape (i.e., phonological) forms in ASL, suggesting that ASL became activated during the processing of English words, a context where ASL was not perceptually relevant. While the form of the bilingual's two languages may influence processing, it does not eliminate non-selectivity entirely. More research is necessary before we can conclude that bilinguals do not exploit language specific features to allow language-specific lexical access.

Aspects of language context

A second potential cue may be present in aspects of the language context and in higher order linguistic representations such as the syntax or semantics in which words are typically embedded. Despite the presence of rich context in naturalistic language use, the early experimental evidence for non-selectivity came almost entirely from tasks in which words were presented in isolation (e.g., Dijkstra et al., 1998; Dijkstra et al., 1999; Schwartz et al., 2007; Van Hell \textbackslash{}\& Dijkstra, 2002). An obvious question was whether sentence context itself would override the nonselectivity observed in isolated word recognition. Quite counterintuitively, recent research suggests that the mere presence of a sentence context alone seems to be ineffective in allowing a bilingual to select one language during comprehension. When bilinguals process language ambiguous words within a coherent sentence context, the effects of the language not in use remains, as if the words had been presented out of context. For example, Van Assche et al. (2009) reported cognate effects while Dutch-English bilinguals read sentences in their native language, Dutch. Although the sentences appeared in only one language and that language was their native and more dominant language, there was a persistent effect of English, the L2, on the processing of Dutch, the L1. Although effects tend to be more robust in the L2 which tends to be more vulnerable to the influence of the L1, the overall pattern of these findings have been replicated in a number of studies with different language pairs.

One aspect of sentence context that does appear to function to accomplish language selection is highly predictable semantic constraint. When a sentence is highly predictable in its interpretation, cross-language effects are diminished to the point where they are no longer observable. For example, Schwartz and Kroll (2006) asked Spanish-English bilinguals to read sentences in which cognates and non-cognate controls were embedded. One set of sentences were low constraint in that the critical target word, a cognate or control, was not predictable on the basis of the initial context. Another set of sentences was highly predicable. To illustrate, in a sentence like \texttt{When we entered the hall we saw a piano in the corner of the room'' the cognate word}piano'' is not predictable given the surrounding context, hence it has a low semantic constraint. When same cognate is placed in the sentence ``Before playing, the composer wiped the keys of the piano at the beginning of the concert it becomes highly predictable given the preceding context. Schwartz and Kroll found that in low constraint sentences, there was cognate facilitation similar to out of context presentation. In contrast, following high semantic constraint, cognate facilitation was eliminated, suggesting that word recognition became language-selective. The results were virtually identical in both English and Spanish. Schwartz (2003) further found that high constraint sentences functioned to eliminate cross-language phonological modulation that had been observed in isolated word recognition (e.g., Schwartz et al., 2007). This type of interaction between semantic constraint and language co-activation has been documented in a handful of other studies (e.g., Chambers \textbackslash{}\& Cooke, 2009; Libben \textbackslash{}\& Titone, 2009; Titone et al., 2011; Van Hell \textbackslash{}\& De Groot, 2008), although there is remaining debate about the presence and locus of the semantic constraint effects (e.g., see Van Assche et al., 2010).

Grammatical information also seem to function as a potential language cue. Sunderman and Kroll (2006) found that lexical form interference could be eliminated in a translation recognition task (i.e., decide whether two words are translations of one another) when the two words differed in their grammatical class. Likewise, Baten, Hofman, and Loeys (2010) demonstrated that word class interacted with the degree of language co-activation for words embedded in a sentence context. A facilitatory homograph effect was present when participants were required to make a lexical decision to target words, but only when the meaning of the homograph shared grammatical class with its translation. For example, when the Dutch-English homograph brief was used as an adjective in an English sentence (brief is a noun meaning letter in Dutch) no homograph interference was observed, suggesting that higher order grammatical properties such as word class can provide information that that can aid bilinguals in selecting the target language. In each of these examples, it is not clear whether the locus of selection occurs early or late in processing. Although more evidence overall suggests a late point of selection, consistent with the predictions of the BIA + model, identifying the precise locus at which selection will require that studies use methods such as eye tracking and ERPs that permit a sensitive analysis of the early time course of processing.

A critical question is whether the apparent language nonselectivity that is evident in bilingual word recognition will also be manifest when words are read in sentence context. Isolated word presentation may increase ambiguity, resulting in exaggerated processing for words that share cross-language overlap. In theory, the language of a sentence context should provide bilinguals with a cue to the target language that effectively reduces the activation of the language not in use. However, it appears that even when words are presented in unilingual sentence contexts, word recognition remains non-selective. When bilinguals are asked to name words in the context of sentences, naming latencies reveal patterns of cross-language activation consistent with out of context presentations (e.g., Schwartz \textbackslash{}\& Kroll, 2006). Eye-movement records within context also indicate that language ambiguous words elicit differential fixation and gaze duration patterns, which are assumed to track the earliest stages of lexical access (e.g., Duyck et al., 2007; Libben \textbackslash{}\& Titone, 2009; Van Assche et al., 2009; Van Assche et al., 2010). These findings are consistent with an interpretation of the BIA+ model in which word recognition proceeds without any influence from the presence of a sentence context.

Given persistent parallel activation of the two languages, a recent question has been whether there are any contextual factors which can function as a cue to allow bilinguals to selectively access, or ``zoom in'' to a single language alone (e.g., Elston-Güttler et al., 2005). To date, the only factor that appears to reduce lexical nonselectivity is semantic constraint. When a sentence contains a highly predictable lexical alternative, then the effects of cross language overlap are decreased or eliminated (e.g., Schwartz \textbackslash{}\& Kroll, 2006; Libben \textbackslash{}\& Titone, 2009; Van Hell \textbackslash{}\& De Groot, 2008; but see Van Assche et al., 2010). Currently, BIA+ does not model the way in which the semantics of a sentence context interact with the non-selectivity of the system. The model could be modified to allow, for example, the semantics of a sentence to preactivate the language nodes, allowing for a faster language selection to occur. However, such an alteration might be drastic given that sentences which are low semantic constraint yet still coherent in their meaning still elicit parallel activation. Clearly, other factors to language selection must be identified before the BIA+ model is modified to account for the influence of linguistic context during word recognition. Curiously, a factor that has received little attention is grammatical structure. Languages differ syntactically and these differences lead to language-specific representations. The hypothesis in the proposed research is that cross-language syntactic differences may function to achieve language selection during word recognition.

\section{What about the syntax?}
\label{whataboutthesyntax}

The question of whether cross-language syntactic differences influence word recognition was addressed by Gullifer et al. (2011). In that study, Spanish-English bilinguals read sentences in each of their languages and named a critical target word aloud. Target words were cognates (e.g., bus in English and Spanish) matched to unambiguous control words (e.g., hairspray-laca). Half of the Spanish sentences contained syntax structurally specific to Spanish. Syntactic specificity was manipulated in two ways: (a) the indirect object of a ditransitive verb was realized pleonastically with the proclitic le and its corresponding noun phrase; and (b) the grammatical subject of the object relative clause was not expressed overtly (e.g., Las monjas (a) le llevaron las mantas que (b)(pro) habían bordado a la directora del orfanato. [The nuns took the quilts that they had embroidered to the director of the orphanage.]) The English translations were controls in that the initial phrase of the sentence was not syntactically specific to either language. When all participants were included in the analyses, there was cognate facilitation that did not depend on the syntax of the sentence. Monolingual speakers of English and Spanish exhibited no cognate effects. However, data from a subset of the bilingual participants who were fastest to perform the naming task revealed the predicted interaction between sentence type and cognate status, suggesting that for these speakers, language-specific syntax eliminated the cognate effect. No independent measure of proficiency clearly modulated the effect. Taken together, the results suggested that bilinguals activate both languages while reading a unilingual sentence. If language-specific syntax did modulate nonselectivity, its effect was subtle. The results of Gullifer et al. raise the question of exactly what types of structures function as language specific. Descriptively, the presence of clitics and pro-drop are Spanish specific in comparison to English, but these (morpho)syntactic features may be too subtle to be exploited by bilinguals during processing. A more robust syntactic manipulation, for example, a structure that is assured to be represented differentially across languages, may function as such a cue that can allow bilinguals to select a language without influence from the unintended language. The question of how syntactic representations are represented and processed has been addressed in the work on cross-language syntactic priming. The presence or absence of syntactic priming has been taken as evidence for shared vs. separate syntactic representations across the two languages of a bilingual. The idea in the planned experiments is to first exploit syntactic priming as a method to differentiate language specific and language shared structures and then to assess the consequences of those structures for restricting lexical access to the language in use.

\chapter{Chapter 2: Roadmap}
\label{chapter2:roadmap}

\section{\textbackslash{}section\{Cross-language syntactic priming\}}
\label{sectioncross-languagesyntacticpriming}

\textbf{Cross-language syntactic priming}

\begin{verbatim}
Syntactic priming is the phenomenon whereby the appearance of a certain syntactic structure facilitates the subsequent production or processing of that structure. Classically, syntactic priming is observed when monolingual participants read a prime sentence (e.g., active: ``The lightning struck the house'' or passive: ``The house was struck by lightning'') and are asked to describe a picture of a novel event (e.g., a man eating an apple). Participants are more likely to describe the picture using the passive voice when the preceding sentence primes the passive voice (Bock, 1986). Since Bock, syntactic priming has been used as a tool to investigate how syntax is processed (see Pickering \& Ferreira, 2008, for a review). Relevant to the current proposal, work on syntactic priming in bilinguals has demonstrated that the syntactic representations of two languages may utilize partially overlapping representations (Schoonbaert et al., 2007) with representational overlap depending on shared word order across languages. Priming is relatively robust for structures that overlap in word order. For example, Loebell and Bock (2003) showed that German (L1) – English (L2) bilinguals elicited cross-language priming for structures in the dative alternation which overlap perfectly in their word order (e.g., double object: The boy sent his pen pal a letter [Der Junge schickte seinem Breiffreund einen Brief]; prepositional dative: The boy sent a letter to his pen pal. [Der Junge schickte einen Brief an seinen Brieffreund]). Similar findings have been shown for the dative alternation in Dutch-English bilinguals (Schoonbaert et al, 2007), Swedish-English bilinguals (Kantola, \& Van Gompel, 2011), and Greek-English bilinguals (Salamoura \& Williams, 2007); the adjective-noun/relative clause alternation in Dutch and German (Bernolet, et al., 2007); as well as with the active/passive alternation in Spanish-English bilinguals (Hartsuiker, et al., 2004) and Polish-English bilinguals (Fleischer et al., 2012). These results suggest that bilingual syntactic representations overlap when word order is shared across the two languages.
\end{verbatim}

Structures that contain word order differences across languages are less likely to show cross-language syntactic priming. Loebell and Bock (2003) observed no syntactic priming across German and English for structures in the active\slash passive alternation (active: The janitor cleans the floors daily. [Der Hausmeister reinigt die Boeden taeglich.]; passive: The floors are cleaned daily by the janitor. [Die Boeden werden taeglich von dem Hausmeister gereinigt.]). In this alternation, the passive structure differs in word order across the two languages because the main verb of the German sentence (``gereinigt'' the past participle of the verb to clean) comes at the end of the clause. Likewise, the adjective-noun\slash relative clause alternation in German and English (relative clauses in German exhibit verb-final structure in contrast to English) does not elicit priming (Bernolet et al., 2007). Salamoura and Williams (2007) found that prepositional object dative constructions that involved word order variations between Greek and English elicited no cross-language syntactic priming. There are cases in which priming is observable across languages despite the presence of word order differences. Desmet and Declercq (2006) found that relative clause attachment sites (e.g., NP1 vs. NP2 attachment) can be primed across Dutch and English despite Dutch a verb-final structure within Dutch relative clauses which differs from that of English relative clauses. Shin and Christianson (2009) showed that priming occurs for the dative alternation between Korean and English despite differing word orders of Korean and English (Korean has SOV word order while English has SVO word order). Weber and Indefrey (2009) reported that passives could be primed between German and English despite word order differences between the constructions. Fleischer et al. (2012) found the Polish active OVS structure primed use of the English passive in Polish-English bilinguals. In one sense, this result suggests that priming can occur despite word order differences between languages because English does not have OVS word order in active sentences. However, the results stress the importance of linear word order because participants were primed to produce a construction (the English passive) in which the linear order of thematic roles overlapped across languages (the passive and OVS both place stress on the grammatical patient by fronting it). Clearly, the distinction between shared and separate syntactic representations as dictated by the presence or absence of word order differences is not clear-cut. However, the presence of discrepancies in the results of experiments utilizing structures with word order differences across languages suggests that the degree of representational overlap is reduced when word order is not shared.

An alternative explanation for the discrepancy in the studies presented above regarding priming and word order differences is that the results are confounded with the type of task being used. Studies that find an asymmetry in priming between overlapping and non-overlapping word orders tend to use the classical picture priming paradigm in which confederate speakers present participants with prime sentences before the participants describe a scene (e.g., Bernolet et al., 2007; Loebell \textbackslash{}\& Bock, 2003) or other production oriented experiments (e.g., Salamoura \textbackslash{}\& Williams, 2007). In contrast, the studies finding evidence for cross-language syntactic priming despite word order differences have used a wider range of tasks including sentence recall and self-paced reading (SPR). There may therefore be differential sensitivity across tasks to detect cross language priming for structures without word order overlap. Comprehension tasks, such as self-paced reading, tend to be less sensitive to the detection of syntactic priming (e.g., Ledoux et al., 2007; Thothathiri \textbackslash{}\& Snedeker, 2008) and typically require lexical repetition to obtain priming. In the present study we assess cross-language priming in both production and comprehension and include ERPs to identify syntactic priming that occurs during comprehension but that may not be observable in behavioral tasks (e.g., Ledoux, et al., 2007; Tooley et al., 2009). 

\textbackslash{}begin\{itemize\}
\textbackslash{}item The case for syntax

\textbackslash{}item Syntax constrains code-switching - insertion of words from a differnt language into the frame
\textbackslash{}item ** Linzen et al. (2013) suggest that syntactic environment in which a word normally occurs influences word recognition in the MEG signal. unambiguous words would be low entropy, entropy increases for ambiguous words, especially those that are balanced in which languages they occur in (exact cognates ). words that have a strong association with a context (low entropy) should be harder to recognize outside of that context. this predicts that noncognates or nonhomographs (unambiguous control words) should be most influenced by conext.
\textbackslash{}item ** Goodman et al. (1981) - syntactically appropriate and inappropriate word pairs. recognition in an LDT was influenced by syntactic appropriatness{\ldots} authors do indicate that it would be hard to completely disentangle syntactic inappropriateness from semantic inappropriateness
\textbackslash{}item ** West and Stanovich (1986) - modal verb contexts followed by main verb targets and preposition contexts followed by noun targets produced faster response times than did the opposite pairings (i.e., modal\slash noun and preposi- tion\slash verb), This occured in naming and LDT.

\textbackslash{}item ** Wright and Garrett (1984)
\textbackslash{}end\{itemize\}

\chapter{Chapter 3: RSVP}
\label{chapter3:rsvp}

\chapter{Chapter 4: Priming}
\label{chapter4:priming}

\chapter{Chapter 5: Discussion}
\label{chapter5:discussion}
\end{document}
